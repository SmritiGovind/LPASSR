{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zG7e-p4IjE9U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710004039453,"user_tz":-330,"elapsed":27351,"user":{"displayName":"SMRITI GOVIND","userId":"14896104765465681722"}},"outputId":"bada4daa-a6c4-4c2a-a3be-5096d97f663b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":664,"status":"ok","timestamp":1710007034181,"user":{"displayName":"SMRITI GOVIND","userId":"14896104765465681722"},"user_tz":-330},"id":"fNv458k0e3RI","outputId":"50e19b4b-5898-4aed-97bc-8b300b7478f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["len(train_dataloader): 800\n"]}],"source":["# dataset\n","\n","import cv2\n","import os\n","import torch\n","import random\n","import numpy as np\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","\n","def create_dataloader(split, SR_rate, augment, batch_size=1, shuffle=False, num_workers=1, pin_memory=True):\n","    dataset = dataread(split, SR_rate, augment)\n","    dataloader = DataLoader(dataset, batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory)\n","    # print('check 0', dataloader) #me\n","    return dataloader\n","\n","def random_crop(LR_img, HR_img, crop_size, SR_rate):\n","    # check the shape\n","    # print('LR_img.shape', LR_img.shape) #me\n","    # print('HR_img.shape', HR_img.shape) #me\n","    LR_h, LR_w = LR_img.shape[:2]\n","    HR_h, HR_w = HR_img.shape[:2]\n","    # print('np.round_(LR_h * SR_rate)=', np.round_(LR_h * SR_rate), ', HR_h=', HR_h)\n","    # assert np.round_(LR_h * SR_rate) == HR_h and np.round_(LR_w * SR_rate) == HR_w, 'SR_rate is not correct for LR and HR image'\n","    # check the crop size\n","    new_LR_h, new_LR_w = crop_size\n","    assert new_LR_h <= LR_h and new_LR_w <= LR_w, 'crop_size is too large'\n","\n","    y1 = random.randint(0, LR_h - new_LR_h)\n","    x1 = random.randint(0, LR_w - new_LR_w)\n","\n","    LR_crop = LR_img[y1:y1 + new_LR_h, x1:x1 + new_LR_w, :]\n","    HR_crop = HR_img[SR_rate * y1:SR_rate * (y1 + new_LR_h), SR_rate * x1:SR_rate * (x1 + new_LR_w), :]\n","\n","    return LR_crop, HR_crop\n","\n","class dataread(Dataset):  # for training/testing\n","    def __init__(self, split, SR_rate, augment=False):\n","\n","        self.split = split\n","        self.SR_rate = SR_rate\n","        self.augment = augment\n","        self.intensity_list = [1.0, 0.7, 0.5]\n","        self.crop_size = [32, 32]\n","\n","        # data split\n","        if split == 'train':\n","           #self.LR_dir = os.path.join('/content/drive/MyDrive/ColabNotebooks/Research_works/dataset/DIV2K/DIV2K_train_LR_bicubic/', 'X'+str(SR_rate))\n","            self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/Train/left')\n","            self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/Train/right')\n","            self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/train/left'\n","            self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/train/right'\n","            self.img_names_l = sorted(os.listdir(self.LR_dir_l))\n","            self.img_names_r = sorted(os.listdir(self.LR_dir_r))\n","            # print('Sorted HR_l images upto 5: ', self.img_names_l)\n","            # print('Sorted HR_r images upto 5: ', self.img_names_r)\n","            # print('LR_dir_l: ', self.LR_dir_l)\n","            # print('LR_dir_r: ', self.LR_dir_r)\n","            # print('HR_dir_l: ',self.HR_dir_l)\n","            # print('HR_dir_r: ',self.HR_dir_r)\n","\n","        elif split == 'valid':\n","            #self.LR_dir = os.path.join('/content/drive/MyDrive/ColabNotebooks/Research_works/dataset/DIV2K/DIV2K_train_LR_bicubic/', 'X'+str(SR_rate))\n","            self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/val/left')\n","            self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/val/right')\n","            self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/val/left'\n","            self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/val/right'\n","            self.img_names_l = sorted(os.listdir(self.HR_dir_l))\n","            self.img_names_r = sorted(os.listdir(self.HR_dir_r))\n","\n","        elif split == 'test':\n","            # self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/test/left')\n","            # self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/LR/X2/separated/test/right')\n","            # self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/test/left'\n","            # self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/Flickr_modifd/HR/size_corrected/hr_x2/test/right'\n","\n","            self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/LR/x2/left')\n","            self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/LR/x2/right')\n","            self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/HR_size_crctd/x2/left'\n","            self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/kitti_mix/HR_size_crctd/x2/right'\n","\n","            # self.LR_dir_l = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Middleburry_mix/LR/x2/left')\n","            # self.LR_dir_r = os.path.join('/content/drive/MyDrive/phd/StereoSR/datasets/Middleburry_mix/LR/x2/right')\n","            # self.HR_dir_l = '/content/drive/MyDrive/phd/StereoSR/datasets/Middleburry_mix/HR_size_corct/x2/left'\n","            # self.HR_dir_r = '/content/drive/MyDrive/phd/StereoSR/datasets/Middleburry_mix/HR_size_corct/x2/right'\n","\n","            self.img_names_l = sorted(os.listdir(self.HR_dir_l))[:19]\n","            self.img_names_r = sorted(os.listdir(self.HR_dir_r))[:19]\n","        else:\n","            raise NameError('data split must be \"train\", \"valid\" or \"test\". ')\n","\n","\n","\n","    def __len__(self):\n","\n","\n","        if self.split == 'train':\n","           return len(self.img_names_l)\n","        else:\n","          return len(self.img_names_r)\n","\n","\n","\n","    def __getitem__(self, index):\n","\n","        if self.split == 'train':\n","            #LR_img = cv2.imread(os.path.join(self.LR_dir, self.img_names[index][:-4]+'x'+str(self.SR_rate)+'.png')) / 255.  #me\n","\n","            #load  LR stereo\n","            LR_path_l = os.path.join(self.LR_dir_l, self.img_names_l[index])\n","            LR_path_r = os.path.join(self.LR_dir_r, self.img_names_r[index])\n","            # print('LR_path_l: ',LR_path_l)\n","            # print('LR_path_r: ',LR_path_r)\n","            # print('type_l: ', type(LR_path_l))\n","            # print('type_r: ', type(LR_path_r))\n","            LR_img1_l = cv2.imread(LR_path_l)  #me\n","            LR_img1_r = cv2.imread(LR_path_r)  #me\n","            # print(type(LR_img1_l))\n","            LR_img_l = LR_img1_l/255\n","            LR_img_r = LR_img1_r/255\n","\n","            #load HR stereo\n","            HR_img_l = cv2.imread(os.path.join(self.HR_dir_l, self.img_names_l[index])) / 255.\n","            HR_img_r = cv2.imread(os.path.join(self.HR_dir_r, self.img_names_r[index])) / 255\n","\n","            if self.augment:\n","                # random crop\n","                LR_img_l, HR_img_l = random_crop(LR_img_l, HR_img_l, self.crop_size, self.SR_rate)\n","                LR_img_r, HR_img_r = random_crop(LR_img_r, HR_img_r, self.crop_size, self.SR_rate)\n","\n","                # geometric transformations\n","                if random.random() < 0.5: # hflip\n","                    LR_img_l, LR_img_r = LR_img_l[:, ::-1, :], LR_img_r[:, ::-1, :]\n","                    HR_img_l, HR_img_r = HR_img_l[:, ::-1, :], HR_img_r[:, ::-1, :]\n","                if random.random() < 0.5: # vflip\n","                    LR_img_l, LR_img_r = LR_img_l[::-1, :, :], LR_img_r[::-1, :, :]\n","                    HR_img_l, HR_img_r = HR_img_l[::-1, :, :], HR_img_r[::-1, :, :]\n","                if random.random() < 0.5: # rot90\n","                    LR_img_l, LR_img_r = LR_img_l.transpose(1, 0, 2), LR_img_r.transpose(1, 0, 2)\n","                    HR_img_l, HR_img_r = HR_img_l.transpose(1, 0, 2), HR_img_r.transpose(1, 0, 2)\n","\n","                # intensity scale\n","                intensity_scale = random.choice(self.intensity_list)\n","                LR_img_l *= intensity_scale\n","                LR_img_r *= intensity_scale\n","                HR_img_l *= intensity_scale\n","                HR_img_r *= intensity_scale\n","\n","\n","        else:\n","            #LR_img = cv2.imread(os.path.join(self.LR_dir, self.img_names[index][:-4]+'x'+str(self.SR_rate)+'.png')) / 255. #me\n","            LR_img_l = cv2.imread(os.path.join(self.LR_dir_l, self.img_names_l[index])) / 255.  #me\n","            LR_img_r = cv2.imread(os.path.join(self.LR_dir_r, self.img_names_r[index])) / 255.\n","            HR_img_l = cv2.imread(os.path.join(self.HR_dir_l, self.img_names_l[index])) / 255.\n","            HR_img_r = cv2.imread(os.path.join(self.HR_dir_r, self.img_names_r[index])) / 255.\n","\n","        # Convert\n","        LR_img_l = np.ascontiguousarray(LR_img_l.transpose(2, 0, 1)) # HWC => CHW\n","        LR_img_r = np.ascontiguousarray(LR_img_r.transpose(2, 0, 1))\n","        HR_img_l = np.ascontiguousarray(HR_img_l.transpose(2, 0, 1))\n","        HR_img_r = np.ascontiguousarray(HR_img_r.transpose(2, 0, 1))\n","\n","        #return torch.from_numpy(LR_img_l), torch.from_numpy(LR_img_r), torch.from_numpy(HR_img_l), torch.from_numpy(HR_img_r), self.img_names_l[index],self.img_names_r[index]\n","        return torch.from_numpy(LR_img_l), torch.from_numpy(LR_img_r), torch.from_numpy(HR_img_l), torch.from_numpy(HR_img_r), self.img_names_l[index],self.img_names_r[index]\n","\n","if __name__ == '__main__':\n","    os.makedirs('/content/drive/MyDrive/phd/wk1/phase1_baseline/output_jnl/x2/test_dataloader', exist_ok=True)\n","    train_dataloader = create_dataloader('train',2, False, batch_size=1, shuffle=False, num_workers=1)\n","    print(f\"len(train_dataloader): {len(train_dataloader)}\")\n","    #LR_img, HR_img, img_names = next(iter(train_dataloader))\n","    iterator = iter(train_dataloader) #me\n","    LR_img_l, LR_img_r, HR_img_l, HR_img_r, img_names_l, img_names_r = next(iterator)\n","    # #LR_img = next(iterator)\n","    # print(f\"LR_img shape: {LR_img_l.size()}\")\n","    # print(f\"LR_img shape: {LR_img_r.size()}\")\n","    # print(f\"HR_img shape: {HR_img_l.size()}\")\n","    # print(f\"HR_img shape: {HR_img_r.size()}\")\n","    # print('left image names in iterator: ', img_names_l, len(img_names_l))\n","    # print('right image names in iterator: ', img_names_r, len(img_names_r))\n","    LR_img_l = LR_img_l[0].numpy().transpose(1, 2, 0)\n","    LR_img_r = LR_img_r[0].numpy().transpose(1, 2, 0)\n","    HR_img_l = HR_img_l[0].numpy().transpose(1, 2, 0)\n","    HR_img_r = HR_img_r[0].numpy().transpose(1, 2, 0)\n","    cv2.imwrite('./test_dataloader/LR_img_l.png', np.uint8(LR_img_l*255))\n","    cv2.imwrite('./test_dataloader/LR_img_r.png', np.uint8(LR_img_r*255))\n","    cv2.imwrite('./test_dataloader/HR_img_l.png', np.uint8(HR_img_l*255))\n","    cv2.imwrite('./test_dataloader/HR_img_r.png', np.uint8(HR_img_r*255))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PxE29Cl1LFi"},"outputs":[],"source":["# visualization\n","\n","import os\n","import cv2\n","\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def save_res(pred_HR_l, pred_HR_r, img_names_l, img_names_r, folder):\n","    '''\n","\n","    Parameters\n","    ----------\n","    preds : List\n","        each pred has a shape of 1x3xHxW. BGR\n","    img_names : List\n","\n","    Returns\n","    -------\n","    None.\n","\n","    '''\n","    # for pred_HR_l, pred_HR_r, img_names_l, img_names_r in zip(pred_HR_l, pred_HR_r, img_names_l, img_names_r):\n","    #     pred_img_l = pred[0].cpu().numpy().transpose(1,2,0)\n","    #     pred_img_r = pred[0].cpu().numpy().transpose(1,2,0)\n","    #     cv2.imwrite(os.path.join(save_dir, img_names_l), np.uint8(pred_img_l*255))\n","    #     cv2.imwrite(os.path.join(save_dir, img_names_r), np.uint8(pred_img_r*255))\n","    # return\n","\n","    for pred_HR_l, pred_HR_r, img_name_l, img_name_r in zip(pred_HR_l, pred_HR_r, img_names_l, img_names_r):\n","        pred_img_l = pred_HR_l[0].detach().cpu().numpy().transpose(1, 2, 0)\n","        pred_img_r = pred_HR_r[0].detach().cpu().numpy().transpose(1, 2, 0)\n","        cv2.imwrite(os.path.join(folder, img_name_l), np.uint8(pred_img_l * 255))\n","        cv2.imwrite(os.path.join(folder, img_name_r), np.uint8(pred_img_r * 255))\n","    return\n","\n","\n","def visualize_training(save_dir):\n","    txt_res = os.path.join(save_dir, 'results.txt')\n","    with open(txt_res, 'r') as f:\n","        info = f.readlines()\n","    epoch, lr, train_loss, valid_loss, psnr = [], [], [], [], []\n","\n","    for line in info[:-1]:\n","        line = line.strip().split('|')\n","        epoch.append(int(line[0].split(':')[1]))\n","        lr.append(float(line[1].split(':')[1]))\n","        train_loss.append(float(line[2].split(':')[1]))\n","        valid_loss.append(float(line[3].split(':')[1]))\n","        psnr.append(float(line[4].split(':')[1]))\n","\n","    fig = plt.figure(figsize=(16, 8), dpi=400)\n","    ax1 = fig.add_subplot(221)\n","    ax2 = fig.add_subplot(222)\n","    ax3 = fig.add_subplot(223)\n","    ax4 = fig.add_subplot(224)\n","    ax1.title.set_text('Training loss')\n","    ax2.title.set_text('Validation loss')\n","    ax3.title.set_text('PSNR (validation)')\n","    ax4.title.set_text('learning rate')\n","    ax1.plot(epoch, train_loss)\n","    ax2.plot(epoch, valid_loss)\n","    ax3.plot(epoch, psnr)\n","    ax4.plot(epoch, lr)\n","    plt.savefig(os.path.join(save_dir, 'results.png'), dpi=200)\n","    return"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zo9WNPf1igf5"},"outputs":[],"source":["# metric\n","\n","\n","import torch\n","\n","def cal_psnr(x, y):\n","    '''\n","    Parameters\n","    ----------\n","    x, y are two tensors has the same shape (1, C, H, W)\n","\n","    Returns\n","    -------\n","    score : PSNR.\n","    '''\n","\n","    mse = torch.mean((x - y) ** 2, dim=[1, 2, 3])\n","    score = - 10 * torch.log10(mse)\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRrHaTqziuKB"},"outputs":[],"source":["# loss\n","\n","import torch\n","from torch import nn\n","\n","class L1Loss(object):\n","    def __call__(self, input, target):\n","        return torch.abs(input - target).mean()\n","\n","class CharbonnierLoss(nn.Module):\n","\n","    def __init__(self, eps=0.01):\n","        super(CharbonnierLoss, self).__init__()\n","        self.eps = eps\n","\n","    def forward(self, pred, gt):\n","        # print(len(pred),'&', len(gt))\n","        loss = torch.sqrt((pred - gt)**2 + self.eps).mean()\n","\n","        return loss.mean()"]},{"cell_type":"code","source":["########################## new model #############################\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math\n","from torch.quantization import QuantStub, DeQuantStub\n","from torchsummary import summary\n","from skimage import morphology\n","\n","class ClippedReLU(nn.Module):\n","    def __init__(self):\n","        super(ClippedReLU, self).__init__()\n","\n","    def forward(self, x):\n","        return x.clamp(min=0., max=1.)\n","\n","class ConvRelu(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, bias=True):\n","        super(ConvRelu, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        # print(\"input tensor shape @ clipped relu =\", x.shape)\n","        x = self.conv(x)\n","        x = self.relu(x)\n","        return x\n","\n","class Gblock(nn.Module):\n","    def __init__(self, in_channels, out_channels, groups):\n","        super(Gblock, self).__init__()\n","        self.conv0 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, groups=groups)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size=1, padding=0)\n","\n","    def forward(self, x):\n","        x = self.conv0(x)\n","        x = self.relu(x)\n","        x = self.conv1(x)\n","        return x\n","\n","class ResB(nn.Module):\n","    def __init__(self, channels):\n","        super(ResB, self).__init__()\n","        self.body = nn.Sequential(\n","            nn.Conv2d(channels, channels, 3, 1, 1, bias=False),\n","            nn.LeakyReLU(0.1, inplace=True),\n","            nn.Conv2d(channels, channels, 3, 1, 1, bias=False),\n","        )\n","    def __call__(self,x):\n","        out = self.body(x)\n","        return out + x\n","\n","class PAM(nn.Module):\n","    def __init__(self, channels):\n","        super(PAM, self).__init__()\n","        self.b1 = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\n","        self.b2 = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\n","        self.b3 = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)\n","        self.softmax = nn.Softmax(-1)\n","        self.rb = ResB(64)\n","        self.fusion = nn.Conv2d(channels * 2 + 1, channels, 1, 1, 0, bias=True)\n","    def __call__(self, x_left, x_right, is_training):\n","        b, c, h, w = x_left.shape\n","        buffer_left = self.rb(x_left)\n","        buffer_right = self.rb(x_right)\n","\n","        ### M_{right_to_left}\n","        Q = self.b1(buffer_left).permute(0, 2, 3, 1)                                                # B * H * W * C\n","        S = self.b2(buffer_right).permute(0, 2, 1, 3)                                               # B * H * C * W\n","        # print(\"Q shape:\", Q.shape)\n","        # print(\"S shape:\", S.shape)\n","        score = torch.bmm(Q.contiguous().view(-1, w, c),\n","                          S.contiguous().view(-1, c, w))                                            # (B*H) * W * W\n","        M_right_to_left = self.softmax(score)\n","\n","        ### M_{left_to_right}\n","        Q = self.b1(buffer_right).permute(0, 2, 3, 1)                                               # B * H * W * C\n","        S = self.b2(buffer_left).permute(0, 2, 1, 3)                                                # B * H * C * W\n","        '''the input tensors Q and S may represent the hidden states of two\n","        different encoders. '''\n","        score = torch.bmm(Q.contiguous().view(-1, w, c),\n","                          S.contiguous().view(-1, c, w))                                            # (B*H) * W * W\n","        '''what Score do? The torch.bmm() operation can be used to calculate the\n","        similarity between the two hidden states (Q,S), which can be used to predict\n","        the relationship between the two input sequences.'''\n","        M_left_to_right = self.softmax(score)\n","        print('attention map:',M_left_to_right.shape)\n","\n","        ### valid masks\n","        V_left_to_right = torch.sum(M_left_to_right.detach(), 1) > 0.1\n","        V_left_to_right = V_left_to_right.view(b, 1, h, w)                                          #  B * 1 * H * W\n","        V_left_to_right = morphologic_process(V_left_to_right)\n","        print(V_left_to_right.shape)\n","        if is_training==1:\n","            V_right_to_left = torch.sum(M_right_to_left.detach(), 1) > 0.1\n","            V_right_to_left = V_right_to_left.view(b, 1, h, w)                                      #  B * 1 * H * W\n","            V_right_to_left = morphologic_process(V_right_to_left)\n","\n","            M_left_right_left = torch.bmm(M_right_to_left, M_left_to_right)\n","            M_right_left_right = torch.bmm(M_left_to_right, M_right_to_left)\n","\n","        ### fusion\n","        buffer = self.b3(x_right).permute(0,2,3,1).contiguous().view(-1, w, c)                      # (B*H) * W * C\n","        buffer = torch.bmm(M_right_to_left, buffer).contiguous().view(b, h, w, c).permute(0,3,1,2)  #  B * C * H * W\n","        out = self.fusion(torch.cat((buffer, x_left, V_left_to_right), 1))\n","        '''This Conv2d layer in 'out' is  used to fuse the original input tensor\n","         with its reflection and a channel of 1s. This helps the network learn\n","         to distinguish between foreground and background pixels in the image.'''\n","\n","        ## output\n","        if is_training == 1:\n","            return out, \\\n","               (M_right_to_left.contiguous().view(b, h, w, w), M_left_to_right.contiguous().view(b, h, w, w)), \\\n","               (M_left_right_left.view(b,h,w,w), M_right_left_right.view(b,h,w,w)), \\\n","               (V_left_to_right, V_right_to_left)\n","        if is_training == 0:\n","            return out\n","\n","def morphologic_process(mask):\n","    device = mask.device\n","    b,_,_,_ = mask.shape\n","    # mask = 1-mask\n","    mask = ~mask\n","    mask_np = mask.cpu().numpy().astype(bool)\n","    mask_np = morphology.remove_small_objects(mask_np, 20, 2)\n","    mask_np = morphology.remove_small_holes(mask_np, 10, 2)\n","    for idx in range(b):\n","        buffer = np.pad(mask_np[idx,0,:,:],((3,3),(3,3)),'constant')\n","        buffer = morphology.binary_closing(buffer, morphology.disk(3))\n","        mask_np[idx,0,:,:] = buffer[3:-3,3:-3]\n","    mask_np = 1-mask_np\n","    mask_np = mask_np.astype(float)\n","\n","    return torch.from_numpy(mask_np).float().to(device)\n","\n","class XLSR_stereo_AM(nn.Module):\n","    def __init__(self, SR_rate):\n","        super(XLSR_stereo_AM, self).__init__()\n","        ### feature extraction\n","        # self.init_feature  = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3,padding=0),\n","        #                                     nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=0),\n","        #                                     ConvRelu(in_channels=16, out_channels=32, kernel_size=1))\n","        self.init_feature  = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3,padding=1),\n","                                            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n","                                           nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n","                                            ConvRelu(in_channels=32, out_channels=64, kernel_size=1))\n","\n","        ### paralax attention\n","        self.pam = PAM(64)\n","\n","        ### Gblock\n","        self.Gblocks = nn.Sequential(Gblock(64, 64, 4),Gblock(64, 64, 4),\n","                                     Gblock(64, 64, 4))\n","\n","        ### paralax attention\n","        # self.conv1 = ConvRelu(in_channels=3, out_channels=32, kernel_size=3)\n","        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n","                                   nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n","                                   nn.LeakyReLU(0.1, inplace=True))\n","\n","        self.pam = PAM(64)\n","\n","        #### upscaling\n","\n","        # self.upscale = nn.Sequential(\n","        #     nn.Conv2d(in_channels=32, out_channels=3*SR_rate**2, kernel_size=3, padding=1),\n","        #     nn.PixelShuffle(SR_rate))\n","        self.upscale = nn.Sequential(\n","            nn.Conv2d(in_channels=64, out_channels=64*SR_rate**2, kernel_size=3, padding=1),\n","            nn.PixelShuffle(SR_rate),\n","            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, padding=1),  # No need for bias=False here\n","            nn.LeakyReLU(0.1, inplace=True),  # Add LeakyReLU activation after the last convolution\n","            nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, padding=1),  # Add one more convolution with padding\n","            )\n","\n","\n","        self.clippedReLU = ClippedReLU()\n","\n","        # weights initialization\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                # nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')\n","                _, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n","                std = math.sqrt(2/fan_out*0.1)\n","                torch.nn.init.normal_(m.weight.data, mean=0, std=std)\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias.data, 0.01)\n","\n","\n","    def forward(self, x_left, x_right, is_training=1):\n","        ### feature extraction\n","        buffer_left = self.init_feature(x_left)\n","        buffer_right = self.init_feature(x_right)\n","\n","        # print(\"buffer_left shape:\", buffer_left.shape)\n","\n","        if is_training == 1:\n","            ### parallax attention\n","            buffer, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","            (V_left_to_right, V_right_to_left) = self.pam(buffer_left, buffer_right, is_training)\n","\n","            # print(\"PAM 1st PAM shape:\", buffer.shape)\n","\n","            ##### G-block\n","            res1 = self.Gblocks(buffer)\n","\n","            # print(\"o/p of Gblock shape:\", res1.shape)\n","\n","            res2_l = self.conv1(x_left)\n","            res2_r = self.conv1(x_right)\n","\n","            # print(\"res2_l shape:\", res2_l.shape)\n","            # print(\"res2_r shape:\", res2_r.shape)\n","\n","            ### parallax attention_left\n","            buffer_l, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","            (V_left_to_right, V_right_to_left) = self.pam(res1, res2_l, is_training)\n","\n","            # print(\"buffer_l shape after 2nd PAM left:\", buffer_l.shape)\n","             ### parallax attention_right\n","            buffer_r, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","            (V_left_to_right, V_right_to_left) = self.pam(res1, res2_r, is_training)\n","            ### upscaling_left\n","            buffer_l = self.clippedReLU(buffer_l)\n","            # print(\"buffer_l shape after 2nd PAM_right and clippedRelu:\", buffer_l.shape)\n","            out_l = self.upscale(buffer_l)\n","            # print(\"out_l shape:\", out_l.shape)\n","            ### upscaling_right\n","            buffer_r = self.clippedReLU(buffer_r)\n","            out_r = self.upscale(buffer_r)\n","\n","            return out_l, out_r, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","                   (V_left_to_right, V_right_to_left)\n","        if is_training == 0:\n","            ### parallax attention\n","            buffer = self.pam(buffer_left, buffer_right, is_training)\n","            #####G-block\n","            res1 = self.Gblocks(buffer)\n","            res2_l = self.conv1(x_left)\n","            res2_r = self.conv1(x_right)\n","            ### parallax attention_left\n","            buffer_l = self.pam(res1, res2_l, is_training)\n","            ### parallax attention_right\n","            buffer_r = self.pam(res1, res2_r, is_training)\n","            ### upscaling_left\n","            buffer_l = self.clippedReLU(buffer_l)\n","            out_l = self.upscale(buffer_l)\n","            ### upscaling_right\n","            buffer_r = self.clippedReLU(buffer_r)\n","            out_r = self.upscale(buffer_r)\n","\n","            return out_l, out_r\n","\n","def print_model_summary(model):\n","    print(\"--------------- Model Summary ---------------\")\n","    total_params = 0\n","    for name, module in model.named_modules():\n","        if isinstance(module, torch.nn.Module):\n","            print(f\"{name:30} -> {str(module):}\")\n","            total_params += sum(p.numel() for p in module.parameters())\n","    print(f\"Total Trainable Parameters: {total_params}\")\n","    print(\"----------------------------------------------\")\n","\n","\n","if __name__ == '__main__':\n","    device = 'cpu'  # Change to 'cpu' if you don't have a GPU\n","    model = XLSR_stereo_AM(3).to(device)\n","    model.eval()\n","\n","    # Create random left and right low-resolution stereo images (batch size = 1)\n","    left_image = torch.randn(1, 3, 32, 32).to(device)\n","    right_image = torch.randn(1, 3, 32, 32).to(device)\n","\n","    # Forward pass through the network\n","    pred = model(left_image, right_image, is_training=1)\n","\n","    print_model_summary(model)\n","\n","    # Unpack the elements of the prediction tuple\n","    if len(pred) == 5:  # Assuming the tuple contains 5 elements as per your model definition\n","        HR_pred_l, HR_pred_r, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","         (V_left_to_right, V_right_to_left) = pred\n","    elif len(pred) == 2:\n","        HR_pred_l, HR_pred_r = pred\n","\n","\n","\n","pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"Total parameters =\", pytorch_total_params)\n","\n","# Print the model summary\n","# summary(model, input_size=[(3, 32, 32), (3, 32, 32)])\n","summary(model, input_size=[(3, 32, 32), (3, 32, 32)])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sGJZDxlxcHt","executionInfo":{"status":"ok","timestamp":1715764393150,"user_tz":-330,"elapsed":1346,"user":{"displayName":"Smrithi Govind","userId":"03773067474377437854"}},"outputId":"6d1fda8c-c48d-46c6-adec-8503983ce576"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["attention map: torch.Size([32, 32, 32])\n","torch.Size([1, 1, 32, 32])\n","attention map: torch.Size([32, 32, 32])\n","torch.Size([1, 1, 32, 32])\n","attention map: torch.Size([32, 32, 32])\n","torch.Size([1, 1, 32, 32])\n","--------------- Model Summary ---------------\n","                               -> XLSR_stereo_AM(\n","  (init_feature): Sequential(\n","    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ConvRelu(\n","      (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (pam): PAM(\n","    (b1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (b2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (b3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (softmax): Softmax(dim=-1)\n","    (rb): ResB(\n","      (body): Sequential(\n","        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n","        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (fusion): Conv2d(129, 64, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (Gblocks): Sequential(\n","    (0): Gblock(\n","      (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","      (relu): ReLU(inplace=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (1): Gblock(\n","      (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","      (relu): ReLU(inplace=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (2): Gblock(\n","      (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","      (relu): ReLU(inplace=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n","  )\n","  (upscale): Sequential(\n","    (0): Conv2d(64, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): PixelShuffle(upscale_factor=3)\n","    (2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): LeakyReLU(negative_slope=0.1, inplace=True)\n","    (4): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  )\n","  (clippedReLU): ClippedReLU()\n",")\n","init_feature                   -> Sequential(\n","  (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): ConvRelu(\n","    (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","    (relu): ReLU(inplace=True)\n","  )\n",")\n","init_feature.0                 -> Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","init_feature.1                 -> Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","init_feature.2                 -> Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","init_feature.3                 -> ConvRelu(\n","  (conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","  (relu): ReLU(inplace=True)\n",")\n","init_feature.3.conv            -> Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n","init_feature.3.relu            -> ReLU(inplace=True)\n","pam                            -> PAM(\n","  (b1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","  (b2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","  (b3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","  (softmax): Softmax(dim=-1)\n","  (rb): ResB(\n","    (body): Sequential(\n","      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n","      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    )\n","  )\n","  (fusion): Conv2d(129, 64, kernel_size=(1, 1), stride=(1, 1))\n",")\n","pam.b1                         -> Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","pam.b2                         -> Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","pam.b3                         -> Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","pam.softmax                    -> Softmax(dim=-1)\n","pam.rb                         -> ResB(\n","  (body): Sequential(\n","    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n",")\n","pam.rb.body                    -> Sequential(\n","  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (1): LeakyReLU(negative_slope=0.1, inplace=True)\n","  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",")\n","pam.rb.body.0                  -> Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","pam.rb.body.1                  -> LeakyReLU(negative_slope=0.1, inplace=True)\n","pam.rb.body.2                  -> Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","pam.fusion                     -> Conv2d(129, 64, kernel_size=(1, 1), stride=(1, 1))\n","Gblocks                        -> Sequential(\n","  (0): Gblock(\n","    (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","    (relu): ReLU(inplace=True)\n","    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (1): Gblock(\n","    (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","    (relu): ReLU(inplace=True)\n","    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","  )\n","  (2): Gblock(\n","    (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","    (relu): ReLU(inplace=True)\n","    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","  )\n",")\n","Gblocks.0                      -> Gblock(\n","  (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","  (relu): ReLU(inplace=True)\n","  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",")\n","Gblocks.0.conv0                -> Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","Gblocks.0.relu                 -> ReLU(inplace=True)\n","Gblocks.0.conv1                -> Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","Gblocks.1                      -> Gblock(\n","  (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","  (relu): ReLU(inplace=True)\n","  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",")\n","Gblocks.1.conv0                -> Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","Gblocks.1.relu                 -> ReLU(inplace=True)\n","Gblocks.1.conv1                -> Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","Gblocks.2                      -> Gblock(\n","  (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","  (relu): ReLU(inplace=True)\n","  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",")\n","Gblocks.2.conv0                -> Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)\n","Gblocks.2.relu                 -> ReLU(inplace=True)\n","Gblocks.2.conv1                -> Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","conv1                          -> Sequential(\n","  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",")\n","conv1.0                        -> Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","conv1.1                        -> Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","conv1.2                        -> LeakyReLU(negative_slope=0.1, inplace=True)\n","upscale                        -> Sequential(\n","  (0): Conv2d(64, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): PixelShuffle(upscale_factor=3)\n","  (2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (3): LeakyReLU(negative_slope=0.1, inplace=True)\n","  (4): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",")\n","upscale.0                      -> Conv2d(64, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","upscale.1                      -> PixelShuffle(upscale_factor=3)\n","upscale.2                      -> Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","upscale.3                      -> LeakyReLU(negative_slope=0.1, inplace=True)\n","upscale.4                      -> Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","clippedReLU                    -> ClippedReLU()\n","Total Trainable Parameters: 1679541\n","----------------------------------------------\n","Total parameters = 496551\n","attention map: torch.Size([64, 32, 32])\n","torch.Size([2, 1, 32, 32])\n","attention map: torch.Size([64, 32, 32])\n","torch.Size([2, 1, 32, 32])\n","attention map: torch.Size([64, 32, 32])\n","torch.Size([2, 1, 32, 32])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 32, 32]             224\n","            Conv2d-2           [-1, 16, 32, 32]           1,168\n","            Conv2d-3           [-1, 32, 32, 32]           4,640\n","            Conv2d-4           [-1, 64, 32, 32]           2,112\n","              ReLU-5           [-1, 64, 32, 32]               0\n","          ConvRelu-6           [-1, 64, 32, 32]               0\n","            Conv2d-7            [-1, 8, 32, 32]             224\n","            Conv2d-8           [-1, 16, 32, 32]           1,168\n","            Conv2d-9           [-1, 32, 32, 32]           4,640\n","           Conv2d-10           [-1, 64, 32, 32]           2,112\n","             ReLU-11           [-1, 64, 32, 32]               0\n","         ConvRelu-12           [-1, 64, 32, 32]               0\n","           Conv2d-13           [-1, 64, 32, 32]          36,864\n","        LeakyReLU-14           [-1, 64, 32, 32]               0\n","           Conv2d-15           [-1, 64, 32, 32]          36,864\n","           Conv2d-16           [-1, 64, 32, 32]          36,864\n","        LeakyReLU-17           [-1, 64, 32, 32]               0\n","           Conv2d-18           [-1, 64, 32, 32]          36,864\n","           Conv2d-19           [-1, 64, 32, 32]           4,160\n","           Conv2d-20           [-1, 64, 32, 32]           4,160\n","          Softmax-21               [-1, 32, 32]               0\n","           Conv2d-22           [-1, 64, 32, 32]           4,160\n","           Conv2d-23           [-1, 64, 32, 32]           4,160\n","          Softmax-24               [-1, 32, 32]               0\n","           Conv2d-25           [-1, 64, 32, 32]           4,160\n","           Conv2d-26           [-1, 64, 32, 32]           8,320\n","           Conv2d-27           [-1, 64, 32, 32]           9,280\n","             ReLU-28           [-1, 64, 32, 32]               0\n","           Conv2d-29           [-1, 64, 32, 32]           4,160\n","           Gblock-30           [-1, 64, 32, 32]               0\n","           Conv2d-31           [-1, 64, 32, 32]           9,280\n","             ReLU-32           [-1, 64, 32, 32]               0\n","           Conv2d-33           [-1, 64, 32, 32]           4,160\n","           Gblock-34           [-1, 64, 32, 32]               0\n","           Conv2d-35           [-1, 64, 32, 32]           9,280\n","             ReLU-36           [-1, 64, 32, 32]               0\n","           Conv2d-37           [-1, 64, 32, 32]           4,160\n","           Gblock-38           [-1, 64, 32, 32]               0\n","           Conv2d-39           [-1, 32, 32, 32]             896\n","           Conv2d-40           [-1, 64, 32, 32]          18,496\n","        LeakyReLU-41           [-1, 64, 32, 32]               0\n","           Conv2d-42           [-1, 32, 32, 32]             896\n","           Conv2d-43           [-1, 64, 32, 32]          18,496\n","        LeakyReLU-44           [-1, 64, 32, 32]               0\n","           Conv2d-45           [-1, 64, 32, 32]          36,864\n","        LeakyReLU-46           [-1, 64, 32, 32]               0\n","           Conv2d-47           [-1, 64, 32, 32]          36,864\n","           Conv2d-48           [-1, 64, 32, 32]          36,864\n","        LeakyReLU-49           [-1, 64, 32, 32]               0\n","           Conv2d-50           [-1, 64, 32, 32]          36,864\n","           Conv2d-51           [-1, 64, 32, 32]           4,160\n","           Conv2d-52           [-1, 64, 32, 32]           4,160\n","          Softmax-53               [-1, 32, 32]               0\n","           Conv2d-54           [-1, 64, 32, 32]           4,160\n","           Conv2d-55           [-1, 64, 32, 32]           4,160\n","          Softmax-56               [-1, 32, 32]               0\n","           Conv2d-57           [-1, 64, 32, 32]           4,160\n","           Conv2d-58           [-1, 64, 32, 32]           8,320\n","           Conv2d-59           [-1, 64, 32, 32]          36,864\n","        LeakyReLU-60           [-1, 64, 32, 32]               0\n","           Conv2d-61           [-1, 64, 32, 32]          36,864\n","           Conv2d-62           [-1, 64, 32, 32]          36,864\n","        LeakyReLU-63           [-1, 64, 32, 32]               0\n","           Conv2d-64           [-1, 64, 32, 32]          36,864\n","           Conv2d-65           [-1, 64, 32, 32]           4,160\n","           Conv2d-66           [-1, 64, 32, 32]           4,160\n","          Softmax-67               [-1, 32, 32]               0\n","           Conv2d-68           [-1, 64, 32, 32]           4,160\n","           Conv2d-69           [-1, 64, 32, 32]           4,160\n","          Softmax-70               [-1, 32, 32]               0\n","           Conv2d-71           [-1, 64, 32, 32]           4,160\n","           Conv2d-72           [-1, 64, 32, 32]           8,320\n","      ClippedReLU-73           [-1, 64, 32, 32]               0\n","           Conv2d-74          [-1, 576, 32, 32]         332,352\n","     PixelShuffle-75           [-1, 64, 96, 96]               0\n","           Conv2d-76            [-1, 3, 96, 96]           1,731\n","        LeakyReLU-77            [-1, 3, 96, 96]               0\n","           Conv2d-78            [-1, 3, 96, 96]              84\n","      ClippedReLU-79           [-1, 64, 32, 32]               0\n","           Conv2d-80          [-1, 576, 32, 32]         332,352\n","     PixelShuffle-81           [-1, 64, 96, 96]               0\n","           Conv2d-82            [-1, 3, 96, 96]           1,731\n","        LeakyReLU-83            [-1, 3, 96, 96]               0\n","           Conv2d-84            [-1, 3, 96, 96]              84\n","================================================================\n","Total params: 1,293,454\n","Trainable params: 1,293,454\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 36.00\n","Forward/backward pass size (MB): 50.69\n","Params size (MB): 4.93\n","Estimated Total Size (MB): 91.62\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwS4a-OBhbMA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"94cb81da-631b-4ab5-cb66-c85e659135ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded pretrained model /content/drive/MyDrive/phd/wk1/phase1_baseline/output_jnl/x2/output3/best.pt\n","Epoch: 1 | lr: 0.00006 | training loss: 0.10734 | validation loss: 0.11235 | PSNR: 25.403 | Time: 1869.1\n","Epoch: 2 | lr: 0.00008 | training loss: 0.10732 | validation loss: 0.11229 | PSNR: 25.445 | Time: 114.7\n","Epoch: 3 | lr: 0.00011 | training loss: 0.10735 | validation loss: 0.11254 | PSNR: 25.335 | Time: 112.2\n","Epoch: 4 | lr: 0.00016 | training loss: 0.10732 | validation loss: 0.11295 | PSNR: 25.124 | Time: 109.0\n","Epoch: 5 | lr: 0.00022 | training loss: 0.10672 | validation loss: 0.11496 | PSNR: 24.381 | Time: 108.9\n","Epoch: 6 | lr: 0.00029 | training loss: 0.10775 | validation loss: 0.11346 | PSNR: 24.898 | Time: 109.6\n","Epoch: 7 | lr: 0.00038 | training loss: 0.10768 | validation loss: 0.11290 | PSNR: 25.134 | Time: 106.6\n","Epoch: 8 | lr: 0.00047 | training loss: 0.10810 | validation loss: 0.11277 | PSNR: 25.213 | Time: 109.9\n","Epoch: 9 | lr: 0.00057 | training loss: 0.10750 | validation loss: 0.11266 | PSNR: 25.319 | Time: 108.6\n","Epoch: 10 | lr: 0.00068 | training loss: 0.10768 | validation loss: 0.11412 | PSNR: 24.685 | Time: 109.1\n","Epoch: 11 | lr: 0.00080 | training loss: 0.10820 | validation loss: 0.11609 | PSNR: 24.004 | Time: 106.6\n","Epoch: 12 | lr: 0.00092 | training loss: 0.10797 | validation loss: 0.11317 | PSNR: 25.011 | Time: 108.3\n","Epoch: 13 | lr: 0.00105 | training loss: 0.10838 | validation loss: 0.11300 | PSNR: 25.100 | Time: 107.7\n","Epoch: 14 | lr: 0.00118 | training loss: 0.10779 | validation loss: 0.11646 | PSNR: 23.866 | Time: 107.0\n","Epoch: 15 | lr: 0.00131 | training loss: 0.10851 | validation loss: 0.11838 | PSNR: 23.181 | Time: 108.2\n","Epoch: 16 | lr: 0.00144 | training loss: 0.11169 | validation loss: 0.12949 | PSNR: 20.914 | Time: 107.9\n","Epoch: 17 | lr: 0.00157 | training loss: 0.11279 | validation loss: 0.11752 | PSNR: 23.443 | Time: 110.5\n","Epoch: 18 | lr: 0.00170 | training loss: 0.10916 | validation loss: 0.11493 | PSNR: 24.328 | Time: 111.0\n","Epoch: 19 | lr: 0.00182 | training loss: 0.10847 | validation loss: 0.11491 | PSNR: 24.267 | Time: 109.9\n","Epoch: 20 | lr: 0.00193 | training loss: 0.10799 | validation loss: 0.11496 | PSNR: 24.349 | Time: 109.1\n","Epoch: 21 | lr: 0.00204 | training loss: 0.10852 | validation loss: 0.12079 | PSNR: 22.551 | Time: 110.1\n","Epoch: 22 | lr: 0.00214 | training loss: 0.10971 | validation loss: 0.11609 | PSNR: 23.984 | Time: 109.6\n","Epoch: 23 | lr: 0.00222 | training loss: 0.10950 | validation loss: 0.11825 | PSNR: 23.202 | Time: 105.4\n","Epoch: 24 | lr: 0.00230 | training loss: 0.11053 | validation loss: 0.12878 | PSNR: 20.927 | Time: 107.5\n","Epoch: 25 | lr: 0.00237 | training loss: 0.11185 | validation loss: 0.12839 | PSNR: 21.103 | Time: 109.0\n","Epoch: 26 | lr: 0.00242 | training loss: 0.11007 | validation loss: 0.11774 | PSNR: 23.447 | Time: 106.0\n","Epoch: 27 | lr: 0.00246 | training loss: 0.10818 | validation loss: 0.11597 | PSNR: 23.961 | Time: 108.6\n","Epoch: 28 | lr: 0.00249 | training loss: 0.10856 | validation loss: 0.11699 | PSNR: 23.626 | Time: 109.0\n","Epoch: 29 | lr: 0.00250 | training loss: 0.10973 | validation loss: 0.12622 | PSNR: 21.429 | Time: 109.9\n","Epoch: 30 | lr: 0.00250 | training loss: 0.11035 | validation loss: 0.12110 | PSNR: 22.532 | Time: 107.5\n","Epoch: 31 | lr: 0.00250 | training loss: 0.11126 | validation loss: 0.12067 | PSNR: 22.741 | Time: 109.8\n","Epoch: 32 | lr: 0.00249 | training loss: 0.11130 | validation loss: 0.12792 | PSNR: 21.111 | Time: 108.2\n","Epoch: 33 | lr: 0.00248 | training loss: 0.11666 | validation loss: 0.13650 | PSNR: 19.813 | Time: 107.3\n","Epoch: 34 | lr: 0.00247 | training loss: 0.11714 | validation loss: 0.14125 | PSNR: 19.047 | Time: 107.8\n","Epoch: 35 | lr: 0.00246 | training loss: 0.11452 | validation loss: 0.14669 | PSNR: 18.248 | Time: 108.2\n","Epoch: 36 | lr: 0.00245 | training loss: 0.11938 | validation loss: 0.13733 | PSNR: 19.601 | Time: 108.7\n","Epoch: 37 | lr: 0.00243 | training loss: 0.12156 | validation loss: 0.17876 | PSNR: 15.589 | Time: 106.0\n","Epoch: 38 | lr: 0.00241 | training loss: 0.13271 | validation loss: 0.16093 | PSNR: 16.795 | Time: 107.0\n","Epoch: 39 | lr: 0.00239 | training loss: 0.12760 | validation loss: 0.15382 | PSNR: 17.596 | Time: 108.4\n","Epoch: 40 | lr: 0.00236 | training loss: 0.12208 | validation loss: 0.13773 | PSNR: 19.522 | Time: 106.0\n","Epoch: 41 | lr: 0.00234 | training loss: 0.11910 | validation loss: 0.13632 | PSNR: 19.766 | Time: 108.2\n","Epoch: 42 | lr: 0.00231 | training loss: 0.12225 | validation loss: 0.14676 | PSNR: 18.190 | Time: 107.3\n","Epoch: 43 | lr: 0.00228 | training loss: 0.12345 | validation loss: 0.14611 | PSNR: 18.299 | Time: 105.8\n","Epoch: 44 | lr: 0.00224 | training loss: 0.12105 | validation loss: 0.13758 | PSNR: 19.547 | Time: 110.3\n","Epoch: 45 | lr: 0.00221 | training loss: 0.11884 | validation loss: 0.13978 | PSNR: 19.145 | Time: 109.4\n","Epoch: 46 | lr: 0.00217 | training loss: 0.11833 | validation loss: 0.13692 | PSNR: 19.640 | Time: 108.3\n","Epoch: 47 | lr: 0.00213 | training loss: 0.11841 | validation loss: 0.13750 | PSNR: 19.543 | Time: 108.5\n","Epoch: 48 | lr: 0.00209 | training loss: 0.11600 | validation loss: 0.13683 | PSNR: 19.696 | Time: 111.3\n","Epoch: 49 | lr: 0.00205 | training loss: 0.11566 | validation loss: 0.13509 | PSNR: 20.027 | Time: 109.1\n","Epoch: 50 | lr: 0.00201 | training loss: 0.11574 | validation loss: 0.13476 | PSNR: 20.013 | Time: 109.7\n","Epoch: 51 | lr: 0.00196 | training loss: 0.11551 | validation loss: 0.13509 | PSNR: 19.972 | Time: 108.4\n","Epoch: 52 | lr: 0.00191 | training loss: 0.11582 | validation loss: 0.13836 | PSNR: 19.480 | Time: 109.6\n","Epoch: 53 | lr: 0.00187 | training loss: 0.11590 | validation loss: 0.13503 | PSNR: 19.955 | Time: 108.7\n","Epoch: 54 | lr: 0.00182 | training loss: 0.11807 | validation loss: 0.15432 | PSNR: 17.296 | Time: 109.1\n","Epoch: 55 | lr: 0.00177 | training loss: 0.11869 | validation loss: 0.13578 | PSNR: 19.778 | Time: 106.9\n","Epoch: 56 | lr: 0.00172 | training loss: 0.11711 | validation loss: 0.13607 | PSNR: 19.818 | Time: 107.4\n","Epoch: 57 | lr: 0.00166 | training loss: 0.11613 | validation loss: 0.13574 | PSNR: 19.878 | Time: 110.3\n","Epoch: 58 | lr: 0.00161 | training loss: 0.11627 | validation loss: 0.13562 | PSNR: 19.874 | Time: 106.5\n","Epoch: 59 | lr: 0.00156 | training loss: 0.11546 | validation loss: 0.13477 | PSNR: 20.053 | Time: 107.7\n","Epoch: 60 | lr: 0.00150 | training loss: 0.11429 | validation loss: 0.13466 | PSNR: 19.996 | Time: 109.0\n","Epoch: 61 | lr: 0.00145 | training loss: 0.11614 | validation loss: 0.13380 | PSNR: 20.197 | Time: 108.5\n","Epoch: 62 | lr: 0.00139 | training loss: 0.11506 | validation loss: 0.13346 | PSNR: 20.260 | Time: 106.5\n","Epoch: 63 | lr: 0.00134 | training loss: 0.11553 | validation loss: 0.13407 | PSNR: 20.131 | Time: 110.4\n","Epoch: 64 | lr: 0.00128 | training loss: 0.11512 | validation loss: 0.13457 | PSNR: 20.065 | Time: 108.7\n","Epoch: 65 | lr: 0.00123 | training loss: 0.11382 | validation loss: 0.13461 | PSNR: 20.057 | Time: 113.3\n","Epoch: 66 | lr: 0.00118 | training loss: 0.11415 | validation loss: 0.13451 | PSNR: 20.069 | Time: 110.8\n","Epoch: 67 | lr: 0.00112 | training loss: 0.11463 | validation loss: 0.13452 | PSNR: 20.008 | Time: 110.5\n","Epoch: 68 | lr: 0.00107 | training loss: 0.11446 | validation loss: 0.13353 | PSNR: 20.208 | Time: 114.1\n","Epoch: 69 | lr: 0.00101 | training loss: 0.11378 | validation loss: 0.13408 | PSNR: 20.116 | Time: 113.8\n","Epoch: 70 | lr: 0.00096 | training loss: 0.11425 | validation loss: 0.13381 | PSNR: 20.142 | Time: 113.3\n","Epoch: 71 | lr: 0.00091 | training loss: 0.11386 | validation loss: 0.13431 | PSNR: 20.089 | Time: 113.6\n","Epoch: 72 | lr: 0.00086 | training loss: 0.11476 | validation loss: 0.13320 | PSNR: 20.244 | Time: 108.2\n","Epoch: 73 | lr: 0.00081 | training loss: 0.11488 | validation loss: 0.13365 | PSNR: 20.213 | Time: 109.0\n","Epoch: 74 | lr: 0.00076 | training loss: 0.11427 | validation loss: 0.13372 | PSNR: 20.132 | Time: 105.6\n","Epoch: 75 | lr: 0.00071 | training loss: 0.11489 | validation loss: 0.13365 | PSNR: 20.192 | Time: 107.7\n","Epoch: 76 | lr: 0.00066 | training loss: 0.11376 | validation loss: 0.13374 | PSNR: 20.134 | Time: 107.7\n","Epoch: 77 | lr: 0.00061 | training loss: 0.11437 | validation loss: 0.13336 | PSNR: 20.233 | Time: 108.7\n","Epoch: 78 | lr: 0.00057 | training loss: 0.11390 | validation loss: 0.13335 | PSNR: 20.251 | Time: 107.6\n","Epoch: 79 | lr: 0.00053 | training loss: 0.11329 | validation loss: 0.13368 | PSNR: 20.163 | Time: 108.2\n","Epoch: 80 | lr: 0.00049 | training loss: 0.11379 | validation loss: 0.13384 | PSNR: 20.180 | Time: 109.4\n","Epoch: 81 | lr: 0.00045 | training loss: 0.11354 | validation loss: 0.13342 | PSNR: 20.213 | Time: 109.0\n","Epoch: 82 | lr: 0.00041 | training loss: 0.11317 | validation loss: 0.13363 | PSNR: 20.142 | Time: 107.6\n","Epoch: 83 | lr: 0.00037 | training loss: 0.11474 | validation loss: 0.13305 | PSNR: 20.288 | Time: 105.4\n","Epoch: 84 | lr: 0.00034 | training loss: 0.11412 | validation loss: 0.13389 | PSNR: 20.180 | Time: 106.8\n","Epoch: 85 | lr: 0.00031 | training loss: 0.11280 | validation loss: 0.13315 | PSNR: 20.262 | Time: 107.4\n","Epoch: 86 | lr: 0.00028 | training loss: 0.11375 | validation loss: 0.13309 | PSNR: 20.255 | Time: 109.5\n"]}],"source":["# train.py\n","\n","import os\n","import yaml\n","import argparse\n","import torch\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","import torch.backends.cudnn as cudnn\n","import time\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","import pandas as pd\n","\n","\n","# flag = True # for storing checkpoints\n","# CP = True   # for storing checkpoints\n","\n","\n","\n","def train(model, dataloader, criteria, device, optimizer, scheduler):\n","    loss_epoch = 0.\n","    criterion_L1 = L1Loss()\n","    for LR_img_l, LR_img_r,HR_img_l, HR_img_r, _ , _ in dataloader:\n","\n","        optimizer.zero_grad()\n","        LR_img_l, LR_img_r = LR_img_l.to(device).float(), LR_img_r.to(device).float()\n","        HR_img_l, HR_img_r = HR_img_l.to(device).float(), HR_img_r.to(device).float()\n","        # print(len(LR_img_l), len(LR_img_r), len(HR_img_l), len(HR_img_r))  ##### check\n","        # HR_pred_l, HR_pred_r = model(LR_img_l, LR_img_r)\n","        HR_pred_l, HR_pred_r, (M_right_to_left, M_left_to_right), (M_left_right_left, M_right_left_right), \\\n","         (V_left_to_right, V_right_to_left) = model(LR_img_l, LR_img_r, is_training = 1)\n","\n","        b, c, h, w = LR_img_l.shape\n","\n","        # print('predct_r & l=',HR_pred_r.shape, '&',HR_pred_l.shape)   ##### check/\n","        # print('gt_r & l=',HR_img_r.shape, '&',HR_img_l.shape)   ##### check\n","        loss_l = criteria(HR_pred_l, HR_img_l)\n","        # loss_r = criteria(HR_pred_r, HR_img_r)\n","        loss_SR = loss_l\n","\n","        ### loss_smoothness\n","        loss_h = criterion_L1(M_right_to_left[:, :-1, :, :], M_right_to_left[:, 1:, :, :]) + \\\n","                   criterion_L1(M_left_to_right[:, :-1, :, :], M_left_to_right[:, 1:, :, :])\n","        loss_w = criterion_L1(M_right_to_left[:, :, :-1, :-1], M_right_to_left[:, :, 1:, 1:]) + \\\n","                   criterion_L1(M_left_to_right[:, :, :-1, :-1], M_left_to_right[:, :, 1:, 1:])\n","        loss_smooth = loss_w + loss_h\n","\n","        ### loss_cycle\n","        Identity = Variable(torch.eye(w, w).repeat(b, h, 1, 1), requires_grad=False).to(device)\n","        loss_cycle = criterion_L1(M_left_right_left * V_left_to_right.permute(0, 2, 1, 3), Identity * V_left_to_right.permute(0, 2, 1, 3)) + \\\n","                         criterion_L1(M_right_left_right * V_right_to_left.permute(0, 2, 1, 3), Identity * V_right_to_left.permute(0, 2, 1, 3))\n","\n","        ### loss_photometric\n","        LR_right_warped = torch.bmm(M_right_to_left.contiguous().view(b*h,w,w), LR_img_r.permute(0,2,3,1).contiguous().view(b*h, w, c))\n","        LR_right_warped = LR_right_warped.view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","        LR_left_warped = torch.bmm(M_left_to_right.contiguous().view(b * h, w, w), LR_img_l.permute(0, 2, 3, 1).contiguous().view(b * h, w, c))\n","        LR_left_warped = LR_left_warped.view(b, h, w, c).contiguous().permute(0, 3, 1, 2)\n","\n","        loss_photo = criterion_L1(LR_img_l * V_left_to_right, LR_right_warped * V_left_to_right) + \\\n","                          criterion_L1(LR_img_r * V_right_to_left, LR_left_warped * V_right_to_left)\n","\n","        ### losses\n","        loss = loss_SR + 0.005 * (loss_photo + loss_smooth + loss_cycle)\n","\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        # print(\"start_step:\", scheduler.last_epoch)   # me\n","        # print(\"end_step:\", scheduler.end_step)       #me\n","        loss_epoch += loss.item()\n","    loss_epoch /= len(dataloader)\n","    lr_epoch = scheduler.get_last_lr()[0]\n","    scheduler.step() # me\n","    return loss_epoch, lr_epoch\n","\n","def validation(model, dataloader, criteria, device):\n","    loss_epoch = 0.\n","    psnr_epoch = 0.\n","    pred_list_l = []\n","    pred_list_r = []\n","    name_list_l = []\n","    name_list_r = []\n","    with torch.no_grad():\n","        for LR_img_l, LR_img_r, HR_img_l, HR_img_r, img_name_l, img_name_r in dataloader:\n","            LR_img_l, LR_img_r = LR_img_l.to(device).float(), LR_img_r.to(device).float()\n","            HR_img_l , HR_img_r =  HR_img_l.to(device).float(), HR_img_r.to(device).float()\n","\n","            HR_pred_l, HR_pred_r = model(LR_img_l, LR_img_r, is_training=0)\n","            SR_left = torch.clamp(HR_pred_l, 0, 1)\n","\n","            # HR_pred_l, HR_pred_r = HR_pred[:, :3, :, :], HR_pred[:, 3:, :, :]\n","            loss_l = criteria(HR_pred_l, HR_img_l)\n","            # loss_r = criteria(HR_pred_r, HR_img_r)\n","            loss = loss_l\n","            loss_epoch += loss.item()\n","            psnr_epoch_l = cal_psnr(HR_pred_l, HR_img_l)\n","            # psnr_epoch_r = cal_psnr(HR_pred_r, HR_img_r)\n","            psnr_batch = psnr_epoch_l\n","            psnr_epoch += psnr_batch.item()\n","            pred_list_l.append(HR_pred_l)\n","            pred_list_r.append(HR_pred_r)\n","            name_list_l += img_name_l\n","            name_list_r += img_name_r\n","    loss_epoch /= len(dataloader)\n","    psnr_epoch /= len(dataloader)\n","    return loss_epoch, psnr_epoch, pred_list_l, pred_list_r, name_list_l, name_list_r\n","\n","save_dir = \"/content/drive/MyDrive/phd/wk1/phase1_baseline/output_jnl/x2/output4\"\n","SR_rate = 2\n","pretrained_model = \"/content/drive/MyDrive/phd/wk1/phase1_baseline/output_jnl/x2/output3/best.pt\"\n","# epochs = 5000\n","epochs = 100\n","batch_size = 16\n","lr_max = 25e-04\n","pct_epoch = 30\n","augment = 'store_true'\n","workers = 2\n","device = 0\n","div_factor = 50.0\n","final_div_factor = 0.5\n","\n","# if os.path.exists(save_dir):\n","#         print(f\"Warning: {save_dir} exists, please delete it manually if it is useless.\")\n","\n","\n","# txt file to record training process in EXCEL\n","# results_df = pd.DataFrame(columns=['Epoch', 'Learning Rate', 'Training Loss',\n","#                                    'Validation Loss', 'PSNR', 'Time'])\n","txt_path = os.path.join(save_dir, 'results.txt')\n","if os.path.exists(txt_path):\n","        os.remove(txt_path)\n","\n","# folder to save the predicted HR image in the validation\n","valid_folder = os.path.join(save_dir, 'valid_res')\n","os.makedirs(valid_folder, exist_ok=True)\n","\n","device = 'cuda'\n","\n","model = XLSR_stereo_AM(SR_rate)\n","#model = PTSQ_quantized_model(opt[1])\n","\n","# load pretrained model        ########################## uncomment if traing needed a restart\n","if pretrained_model.endswith('.pt') and os.path.exists(pretrained_model):\n","        # filter conv4 weights which have different conv channels\n","        model_dict = model.state_dict()\n","        pretrained_dict = torch.load(pretrained_model)\n","        filtered_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n","        model_dict.update(filtered_dict)\n","        model.load_state_dict(model_dict)\n","        print(f\"Loaded pretrained model {pretrained_model}\" )\n","\n","\n","model.to(device)\n","\n","train_dataloader = create_dataloader('train', SR_rate, augment, batch_size,\n","                                     shuffle=True, num_workers=workers)\n","batch = next(iter(train_dataloader))\n","\n","\n","valid_dataloader = create_dataloader('valid', SR_rate, False, 1,\n","                                     shuffle=True, num_workers=1)\n","\n","criteria = CharbonnierLoss()\n","optimizer = optim.Adam(model.parameters(), lr=lr_max/div_factor, betas=(0.9, 0.999), eps=1e-08)\n","\n","\n","scheduler = lr_scheduler.OneCycleLR(optimizer, lr_max, epochs=epochs, steps_per_epoch=len(train_dataloader), pct_start=pct_epoch/epochs, anneal_strategy='cos', \\\n","                                        cycle_momentum=False, div_factor=div_factor, final_div_factor=final_div_factor)\n","\n","\n","epoch_start = 1\n","best_psnr = 0.\n","for idx in range(epoch_start, epochs+1):\n","        t0 = time.time()\n","        train_loss_epoch, lr_epoch = train(model, train_dataloader, criteria, device, optimizer, scheduler)\n","        t1 = time.time()\n","        valid_loss_epoch, psnr_epoch, pred_HR_l, pred_HR_r, img_names_l, img_names_r = validation(model, valid_dataloader, criteria, device)\n","        t2 = time.time()\n","\n","        # result_row = {\n","        # 'Epoch': idx,\n","        # 'Learning Rate': lr_epoch,\n","        # 'Training Loss': train_loss_epoch,\n","        # 'Validation Loss': valid_loss_epoch,\n","        # 'PSNR': psnr_epoch,\n","        # 'Time': t2 - t0 }\n","        # results_df = results_df.concat(result_row, ignore_index=True)\n","\n","        print(f\"Epoch: {idx} | lr: {lr_epoch:.5f} | training loss: {train_loss_epoch:.5f} | validation loss: {valid_loss_epoch:.5f} | PSNR: {psnr_epoch:.3f} | Time: {t2-t0:.1f}\")\n","        with open(txt_path, 'a') as f:\n","            f.write(f\"Epoch: {idx} | lr: {lr_epoch:.5f} | training loss: {train_loss_epoch:.5f} | validation loss: {valid_loss_epoch:.5f} | PSNR: {psnr_epoch:.3f} | Time: {t2-t0:.1f}\" +'\\n')\n","\n","        if psnr_epoch > best_psnr:\n","            best_psnr = psnr_epoch\n","\n","            torch.save(model.state_dict(), os.path.join(save_dir, 'best.pt'))\n","            # save predicted HR image on validation set\n","            save_res(pred_HR_l,pred_HR_r, img_names_l,img_names_r, valid_folder)\n","        del pred_HR_l, pred_HR_r\n","\n","\n","#  # Save the DataFrame to an Excel file after the loop\n","# excel_file_path = os.path.join(save_dir, 'results.xlsx')\n","# results_df.to_excel(excel_file_path, index=False)\n","\n","#  # visualize the training process\n","# visualize_training(save_dir)\n","# print(f\"Training is finished, the best PSNR is {best_psnr:.3f}\")\n","# with open(excel_file_path, 'a') as f:\n","#             f.write(f\"Training is finished, the best PSNR is {best_psnr:.3f}\")\n","\n"," # visualize the training process\n","visualize_training(save_dir)\n","print(f\"Training is finished, the best PSNR is {best_psnr:.3f}\")\n","with open(txt_path, 'a') as f:\n","            f.write(f\"Training is finished, the best PSNR is {best_psnr:.3f}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DCLwE2ciGW8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710007140425,"user_tz":-330,"elapsed":99649,"user":{"displayName":"SMRITI GOVIND","userId":"14896104765465681722"}},"outputId":"75313a75-14f6-4816-a689-5c46601ea537"},"outputs":[{"output_type":"stream","name":"stdout","text":["19\n","Start the inference ...\n","PSRN on ['000_L.png'] and ['000_R.png'] : 26.731, inference time: 817.64ms\n","PSRN on ['001_L.png'] and ['001_R.png'] : 29.520, inference time: 196.35ms\n","PSRN on ['002_L.png'] and ['002_R.png'] : 28.446, inference time: 738.49ms\n","PSRN on ['003_L.png'] and ['003_R.png'] : 30.178, inference time: 219.79ms\n","PSRN on ['004_L.png'] and ['004_R.png'] : 30.421, inference time: 728.56ms\n","PSRN on ['005_L.png'] and ['005_R.png'] : 29.493, inference time: 212.54ms\n","PSRN on ['006_L.png'] and ['006_R.png'] : 29.098, inference time: 215.13ms\n","PSRN on ['007_L.png'] and ['007_R.png'] : 26.588, inference time: 201.05ms\n","PSRN on ['008_L.png'] and ['008_R.png'] : 28.440, inference time: 234.97ms\n","PSRN on ['009_L.png'] and ['009_R.png'] : 29.021, inference time: 745.72ms\n","PSRN on ['010_L.png'] and ['010_R.png'] : 28.384, inference time: 197.23ms\n","PSRN on ['011_L.png'] and ['011_R.png'] : 27.690, inference time: 197.08ms\n","PSRN on ['012_L.png'] and ['012_R.png'] : 31.332, inference time: 209.69ms\n","PSRN on ['013_L.png'] and ['013_R.png'] : 28.096, inference time: 284.97ms\n","PSRN on ['014_L.png'] and ['014_R.png'] : 30.576, inference time: 212.16ms\n","PSRN on ['015_L.png'] and ['015_R.png'] : 28.415, inference time: 224.42ms\n","PSRN on ['016_L.png'] and ['016_R.png'] : 30.444, inference time: 212.57ms\n","PSRN on ['017_L.png'] and ['017_R.png'] : 28.273, inference time: 216.42ms\n","PSRN on ['018_L.png'] and ['018_R.png'] : 29.124, inference time: 198.89ms\n","Average PSRN: 28.962, average inference time: 329.67ms\n","Saving the predicted HR images\n","Testing is done!, predicted HR images are saved in /content/drive/MyDrive/phd/wk1/phase1_baseline/output_jnl/x2/kitti_final/test_res\n"]}],"source":["# test\n","\n","import os\n","import argparse\n","import torch\n","import time\n","\n","#from model import XLSR\n","#from dataset import create_dataloader\n","#from metric import cal_psnr\n","#from visualization import save_res\n","\n","\n","def test(model, dataloader, device, txt_path):\n","    pred_list_l = []\n","    pred_list_r = []\n","    name_list_l = []\n","    name_list_r = []\n","    avg_psnr = 0.\n","    avg_time = 0.\n","    with torch.no_grad():\n","        # print(\"warm up ...\")\n","        random_input_l = torch.randn(1, 3, 640, 360).to(device)\n","        random_input_r = torch.randn(1, 3, 640, 360).to(device)\n","        # # warm up\n","        # for _ in range(10):\n","        #     model(random_input_l, random_input_r, is_training=0 )\n","\n","        # with torch.autograd.profiler.profile() as prof:\n","        #     model(random_input_l, random_input_r, is_training=0)\n","        # print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n","\n","        # print(\"Start testing the model speed on 640*360 input ...\")\n","        # test_t = 0.\n","        # for idx in range(100):\n","        #     if device != 'cpu':\n","        #         torch.cuda.synchronize()\n","        #     t0 = time.perf_counter()\n","        #     model(random_input_l, random_input_r, is_training=0)\n","        #     if device != 'cpu':\n","        #         torch.cuda.synchronize()\n","        #     t1 = time.perf_counter()\n","        #     print(f\"Inference #{idx}, inference time: {1000*(t1-t0):.2f}ms\")\n","        #     test_t += t1 - t0\n","        # print(f\"Average inference time on 640*360 input: {1000*test_t/100:.2f}ms\")\n","        # with open(txt_path, 'a') as f:\n","        #     f.write(f\"Average inference time on 640*360 input: {1000*test_t/100:.2f}ms\" + '\\n')\n","\n","        print(\"Start the inference ...\")\n","        for LR_img_l, LR_img_r, HR_img_l, HR_img_r, img_nam_l , img_nam_r in dataloader:\n","            LR_img_l, LR_img_r = LR_img_l.to(device).float(), LR_img_r.to(device).float()\n","            HR_img_l, HR_img_r = HR_img_l.to(device).float(), HR_img_r.to(device).float()\n","            if device != 'cpu':\n","                torch.cuda.synchronize()\n","            t0 = time.perf_counter()\n","            # print('LR shape = ', LR_img_l.shape,LR_img_r.shape )\n","            HR_pred_l, HR_pred_r = model(LR_img_l, LR_img_r, is_training=0)\n","            # HR_pred_l, HR_pred_r = HR_pred[:, :3, :, :], HR_pred[:, 3:, :, :]\n","            # print('HR_Predictions shape = ', HR_pred_l.shape,HR_pred_r.shape )\n","            if device != 'cpu':\n","                torch.cuda.synchronize()\n","            t1 = time.perf_counter()\n","            psnr_l = cal_psnr(HR_pred_l, HR_img_l).item()\n","            # psnr_r = cal_psnr(HR_pred_r, HR_img_r).item()\n","            psnr = psnr_l\n","            inference_time = t1 - t0\n","            print(f\"PSRN on {img_nam_l} and {img_nam_r} : {psnr:.3f}, inference time: {1000*inference_time:.2f}ms\")\n","            with open(txt_path, 'a') as f:\n","                f.write(f\"PSRN on {img_nam_l} and {img_nam_r} : {psnr:.3f}, inference time: {1000*inference_time:.2f}ms\" + '\\n')\n","            avg_psnr += psnr\n","            avg_time += inference_time\n","            pred_list_l.append(HR_pred_l)\n","            pred_list_r.append(HR_pred_r)\n","            name_list_l += img_nam_l\n","            name_list_r += img_nam_r\n","    avg_psnr /= len(test_dataloader)\n","    avg_time /= len(test_dataloader)\n","    print(f\"Average PSRN: {avg_psnr:.3f}, average inference time: {1000*avg_time:.2f}ms\")\n","    with open(txt_path, 'a') as f:\n","        f.write(f\"Average PSRN: {avg_psnr:.3f}, average inference time: {1000*avg_time:.2f}ms\")\n","    return pred_list_l,pred_list_r, name_list_l, name_list_r\n","\n","'''\n","if __name__ == '__main__':\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--save-dir', type=str, default='exp/OneCyclicLR_exp0', help='hyperparameters path')\n","    parser.add_argument('--SR-rate', type=int, default=3, help='the scale rate for SR')\n","    parser.add_argument('--model', type=str, default='', help='the path to the saved model')\n","    parser.add_argument('--device', type=str, default='cpu', help='gpu id or \"cpu\"')\n","    opt = parser.parse_args()\n","'''\n","save_dir = '/content/drive/MyDrive/phd/wk1/phase1_baseline/output_jnl/x2/kitti_final'\n","SR_rate = 2\n","# model_path = '/content/drive/MyDrive/phd/wk1_in_smriti_07_23_gmail/try2/output5_2/best.pt'\n","model_path = '/content/drive/MyDrive/phd/wk1/phase1_baseline/output_jnl/x2/output4/best.pt'\n","device1 = 'cuda'\n","opt = [save_dir, SR_rate, model, device1]\n","\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# cuDnn configurations\n","\n","if device1 != 'cpu':\n","        torch.backends.cudnn.benchmark = True\n","        torch.backends.cudnn.deterministic = True\n","\n","# txt file to record training process\n","txt_path = os.path.join(save_dir, 'test_res.txt')\n","if os.path.exists(txt_path):\n","        os.remove(txt_path)\n","\n","    # folder to save the predicted HR image in the validation\n","test_folder = os.path.join(save_dir, 'test_res')\n","os.makedirs(test_folder, exist_ok=True)\n","\n","# device = 'cuda:' + str(device1) if device1 != 'cpu' else 'cpu'\n","device = device1\n","model = XLSR_stereo_AM(SR_rate)\n","\n","# load pretrained model\n","if model_path.endswith('.pt') and os.path.exists(model_path):\n","        # model.load_state_dict(torch.load(model, map_location=device))\n","        model.load_state_dict(torch.load(model_path))\n","else:\n","        model.load_state_dict(torch.load(os.path.join(save_dir, 'best.pt'), map_location=device))\n","model.to(device)\n","model.eval()\n","\n","test_dataloader = create_dataloader('test', SR_rate, False, batch_size=1, shuffle=False, num_workers=1)\n","print(len(test_dataloader))\n"," # evaluate\n","# pred_HR_l,pred_HR_r, img_names_l,img_names_r, valid_folder = test(model, test_dataloader, device, txt_path)\n","pred_HR_l,pred_HR_r, img_names_l,img_names_r = test(model, test_dataloader, device, txt_path)\n","\n","print(\"Saving the predicted HR images\")\n","save_res(pred_HR_l,pred_HR_r, img_names_l,img_names_r, test_folder)\n","print(f\"Testing is done!, predicted HR images are saved in {test_folder}\")"]}],"metadata":{"colab":{"collapsed_sections":["nTFip6EfdzK5"],"provenance":[{"file_id":"131KZpkCKEZehTfOTxaZKwNPFv4vc-uo-","timestamp":1691311932071},{"file_id":"1_ybp1OzG1u3v3w2xQyyPsQSEPS8He0L9","timestamp":1689310796413},{"file_id":"1nLij8NtEpH6mieiftPDjXYnQpYrXrvUh","timestamp":1687593635726}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}